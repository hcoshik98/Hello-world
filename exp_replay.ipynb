{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "exp_replay.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hcoshik98/R_Learning/blob/master/exp_replay.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YITI9oD6Rlpr",
        "colab_type": "code",
        "outputId": "b45f4e93-7831-4a67-f173-3c58ac65477a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        }
      },
      "source": [
        "!pip install gym[box2d]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gym[box2d] in /usr/local/lib/python3.6/dist-packages (0.10.11)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym[box2d]) (1.16.4)\n",
            "Requirement already satisfied: pyglet>=1.2.0 in /root/.local/lib/python3.6/site-packages (from gym[box2d]) (1.3.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym[box2d]) (1.12.0)\n",
            "Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.6/dist-packages (from gym[box2d]) (2.21.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym[box2d]) (1.3.1)\n",
            "Requirement already satisfied: box2d-py>=2.3.5; extra == \"box2d\" in /usr/local/lib/python3.6/dist-packages (from gym[box2d]) (2.3.8)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet>=1.2.0->gym[box2d]) (0.16.0)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym[box2d]) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym[box2d]) (2019.6.16)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym[box2d]) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym[box2d]) (1.24.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHWr8_zDSQaE",
        "colab_type": "code",
        "outputId": "54e8f400-691d-4898-97fe-fa1cf0a0a478",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        }
      },
      "source": [
        "!apt-get install python-opengl -y"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "python-opengl is already the newest version (3.1.0+dfsg-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 14 not upgraded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1I2n4zy_SXTf",
        "colab_type": "code",
        "outputId": "b7f03ef1-4b2a-42df-d0a2-5a94c8cff5e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        }
      },
      "source": [
        "!apt install xvfb -y"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "xvfb is already the newest version (2:1.19.6-1ubuntu4.3).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 14 not upgraded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ucj3K5RSf8l",
        "colab_type": "code",
        "outputId": "e38a127c-03e5-40aa-a542-2c91cf6a5af6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "!pip3 install pyvirtualdisplay"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyvirtualdisplay in /usr/local/lib/python3.6/dist-packages (0.2.4)\n",
            "Requirement already satisfied: EasyProcess in /usr/local/lib/python3.6/dist-packages (from pyvirtualdisplay) (0.2.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zng3yEzTSqDp",
        "colab_type": "code",
        "outputId": "68be4658-57bc-43be-c017-36108a0f733f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        }
      },
      "source": [
        "!pip install piglet"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: piglet in /usr/local/lib/python3.6/dist-packages (0.4.4)\n",
            "Requirement already satisfied: astunparse in /usr/local/lib/python3.6/dist-packages (from piglet) (1.6.2)\n",
            "Requirement already satisfied: Parsley in /usr/local/lib/python3.6/dist-packages (from piglet) (1.3)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.6/dist-packages (from piglet) (19.1.0)\n",
            "Requirement already satisfied: markupsafe in /usr/local/lib/python3.6/dist-packages (from piglet) (1.1.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.6/dist-packages (from astunparse->piglet) (0.33.4)\n",
            "Requirement already satisfied: six<2.0,>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from astunparse->piglet) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHskRiYJiaQN",
        "colab_type": "code",
        "outputId": "8f7f1bd6-a5c3-4888-c062-56e509fc407e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "#!pip uninstall pyglet\n",
        "!pip install --user pyglet==1.3.2"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyglet==1.3.2 in /root/.local/lib/python3.6/site-packages (1.3.2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet==1.3.2) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAO1Pu3oSvvA",
        "colab_type": "code",
        "outputId": "d24f5987-8d24-4ab9-87f8-fea62a93e343",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "xdpyinfo was not found, X start can not be checked! Please install xdpyinfo!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Display cmd_param=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '1400x900x24', ':1005'] cmd=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '1400x900x24', ':1005'] oserror=None return_code=None stdout=\"None\" stderr=\"None\" timeout_happened=False>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "omhmxArqS6ci",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import piglet\n",
        "import pyglet\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# This code creates a virtual display to draw game images on. \n",
        "# If you are running locally, just ignore it\n",
        "import os\n",
        "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\"))==0:\n",
        "    !bash ../xvfb start\n",
        "    %env DISPLAY=:1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZgwBG7GvTGuv",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "**OpenAI Gym**\n",
        "\n",
        "We're gonna spend several next weeks learning algorithms that solve decision processes. We are then in need of some interesting decision problems to test our algorithms.\n",
        "\n",
        "That's where OpenAI gym comes into play. It's a python library that wraps many classical decision problems including robot control, videogames and board games.\n",
        "\n",
        "So here's how it works:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7L4OrcvS_qK",
        "colab_type": "code",
        "outputId": "8294019e-fdcf-47bf-b8ee-2608f1e8f4f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "import gym\n",
        "env = gym.make(\"LunarLander-v2\")\n",
        "\n",
        "plt.imshow(env.render('rgb_array'))\n",
        "\n",
        "state_dim=env.observation_space.shape\n",
        "print(\"Observation space dim:\", state_dim)\n",
        "n_action= env.action_space.n\n",
        "print(\"Action space shape:\", n_action)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Observation space dim: (8,)\n",
            "Action space shape: 4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAD8CAYAAAB9y7/cAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nVMOrkawAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jz0VQUIbUQYI",
        "colab_type": "text"
      },
      "source": [
        "reset() - reset environment to initial state, return first observation\n",
        "\n",
        "render() - show current environment state (a more colorful version :) )\n",
        "\n",
        "step(a) - commit action a and return (new observation, reward, is done, info)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Enw8EtiQTj1E",
        "colab_type": "code",
        "outputId": "e7c2b38c-26e9-48db-dda8-d8d82acdb4ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "obs0 = env.reset()\n",
        "print(\"initial observation code:\", obs0)\n",
        "\n",
        "# Note: in MountainCar, observation is just two numbers: car position and velocity"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "initial observation code: [ 0.00283756  1.4184152   0.28738642  0.3331092  -0.00328112 -0.06509735\n",
            "  0.          0.        ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDSKvE2fUb_J",
        "colab_type": "code",
        "outputId": "0ae44d68-346e-4ff8-ce51-00000eda38ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        }
      },
      "source": [
        "print(\"taking action 2 (right)\")\n",
        "new_obs, reward, is_done, _ = env.step(2)\n",
        "\n",
        "print(\"new observation code:\", new_obs)\n",
        "print(\"reward:\", reward)\n",
        "print(\"is game over?:\", is_done)\n",
        "\n",
        "# Note: as you can see, the car has moved to the riht slightly (around 0.0005)\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "import keras.layers as L\n",
        "tf.reset_default_graph()\n",
        "sess=tf.InteractiveSession()\n",
        "keras.backend.set_session(sess)\n",
        "saver = tf.train.Saver(max_to_keep=2, keep_checkpoint_every_n_hours=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "taking action 2 (right)\n",
            "new observation code: [ 0.00552874  1.4259039   0.2730865   0.33282274 -0.00720752 -0.07853463\n",
            "  0.          0.        ]\n",
            "reward: -0.49964835178214456\n",
            "is game over?: False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "otBt3YuVfUhs",
        "colab_type": "code",
        "outputId": "1fcee12b-4377-4c48-85d9-93ac8d04a38d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "agent=keras.models.Sequential()\n",
        "print((2*state_dim[0],))\n",
        "Layer1=agent.add(L.InputLayer((2*state_dim[0],)))\n",
        "Layer2=agent.add(L.Dense(100,activation='relu'))\n",
        "Layer3=agent.add(L.Dense(100,activation='relu'))\n",
        "Layer4=agent.add(L.Dense(n_action ,activation='softmax'))\n",
        "\n",
        "agent2=keras.models.Sequential()\n",
        "Layer12=agent2.add(L.InputLayer((2*state_dim[0],)))\n",
        "Layer22=agent2.add(L.Dense(100,activation='relu'))\n",
        "Layer32=agent2.add(L.Dense(100,activation='relu'))\n",
        "Layer42=agent2.add(L.Dense(n_action ,activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0827 14:25:09.914318 140587525711744 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0827 14:25:09.916687 140587525711744 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0827 14:25:09.930177 140587525711744 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(16,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aF9UZOr49FQo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_action(s, epsilon):\n",
        "  q_values=agent.predict(s[None])[0]\n",
        "  q_values2=agent2.predict(s[None])[0]\n",
        "  \n",
        "  expl=np.random.random()\n",
        "  if expl<epsilon:\n",
        "    action= np.random.choice(n_action,1)[0]\n",
        "  else:\n",
        "    action=np.argmax(q_values + q_values2)\n",
        "  return action\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJkLl2Kh-Cwc",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gOo1MIEC-DYi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#create placeholder for training\n",
        "state_ph= tf.placeholder('float32', shape=(None,)+ (2*state_dim[0],))\n",
        "action_ph= tf.placeholder('int32', shape=[None])\n",
        "reward_ph= tf.placeholder('float32', shape=[None])\n",
        "next_st_q= tf.placeholder('float32', shape=[None])\n",
        "next_st_q2= tf.placeholder('float32', shape=[None])\n",
        "next_state_ph= tf.placeholder('float32', shape=(None,)+ (2*state_dim[0],))\n",
        "is_done_ph= tf.placeholder('bool', shape=[None])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_Hj2FOcAS_E",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqBDaCyaATpu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred_q= agent(state_ph)\n",
        "pred_q2= agent2(state_ph)\n",
        "\n",
        "pred_qvalues1 = tf.reduce_sum(pred_q*tf.one_hot(action_ph, n_action), axis=1)\n",
        "pred_qvalues2 = tf.reduce_sum(pred_q2*tf.one_hot(action_ph, n_action), axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqHf6p0lD3pp",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIRIPOmZD4Gd",
        "colab_type": "code",
        "outputId": "e1e25e3b-6b21-4289-95c4-7d553af31831",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 93
        }
      },
      "source": [
        "gamma=0.7\n",
        "#q_values for next state\n",
        "#next_st_q= agent(next_state_ph)\n",
        "#next_st_q2= agent2(next_state_ph)\n",
        "target_q= reward_ph  + gamma*next_st_q#(tf.argmax(next_st_q2))\n",
        "target_q2= reward_ph + gamma*next_st_q2#(tf.argmax(next_st_q2))\n",
        "\n",
        "target_q = tf.where(is_done, reward_ph, target_q)\n",
        "target_q2 = tf.where(is_done, reward_ph, target_q2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0827 14:25:10.092608 140587525711744 deprecation.py:323] From <ipython-input-16-f13582f075ce>:8: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98HMaR3HFg4J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss1= (tf.stop_gradient(target_q)- pred_qvalues1)**2\n",
        "loss1= tf.reduce_mean(loss1)\n",
        "loss2= (tf.stop_gradient(target_q2)- pred_qvalues2)**2\n",
        "loss2= tf.reduce_mean(loss2)\n",
        "\n",
        "train_step1= tf.train.AdamOptimizer(1e-4).minimize(loss1)\n",
        "train_step2= tf.train.AdamOptimizer(1e-4).minimize(loss2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21GHzMahU2fs",
        "colab_type": "text"
      },
      "source": [
        "# Time to play(Try fastai also)\n",
        "\n",
        "Try tweaking t or action strategy\n",
        "\n",
        "**Remember**: Target is to reach to flag"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jnrzvKnBKMq6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_session(t_max=500, epsilon=0, train=True):\n",
        "  import time\n",
        "  total_reward=0\n",
        "  s = env.reset()\n",
        "  see = 0\n",
        "  pre_st= [0, 0, 0, 0, 0, 0, 0, 0]\n",
        "  exp = []\n",
        "  pre = np.empty((1,16))\n",
        "  new = np.empty((1,16))\n",
        "\n",
        "  for t in range(t_max):\n",
        "    pre = np.concatenate((pre_st,s))\n",
        "    a=get_action(pre, epsilon)\n",
        "    \n",
        "    next_s,r,is_done,_ = env.step(a)\n",
        "    new = np.concatenate((s, next_s))\n",
        "    q1= agent.predict(new[None])[0]\n",
        "    q2= agent2.predict(new[None])[0]\n",
        "    qa1= q1[np.argmax(q2)]\n",
        "    qa2= q2[np.argmax(q1)]\n",
        "    exp.append(([pre], [a], [r], [qa1], [qa2], [new]))\n",
        "    if t >=100:\n",
        "      exp.pop(0)\n",
        "    \n",
        "    if train:\n",
        "      idx= [int(np.random.randint(0,len(exp), size=1)) for _ in range(round(len(exp)/4))]\n",
        "      for i in idx:\n",
        "        prob=np.random.random()\n",
        "        if prob<0.8:\n",
        "          sess.run(train_step1, {state_ph: exp[i][0], action_ph: exp[i][1], reward_ph: exp[i][2],next_st_q:exp[i][3],\n",
        "                               next_st_q2:exp[i][4], next_state_ph: exp[i][5], is_done_ph: [is_done]})\n",
        "        else:\n",
        "          sess.run(train_step2, {state_ph: exp[i][0], action_ph: exp[i][1], reward_ph: exp[i][2],next_st_q:exp[i][3],\n",
        "                               next_st_q2:exp[i][4], next_state_ph: exp[i][5], is_done_ph: [is_done]})\n",
        "\n",
        "    total_reward+=r\n",
        "    pre_st=s\n",
        "    s=next_s\n",
        "    if t%10==0:\n",
        "      '''fig=plt.imshow(env.render('rgb_array'))\n",
        "      plt.show(fig)\n",
        "      time.sleep(0.01)\n",
        "      plt.close('all')'''\n",
        "    if is_done:  \n",
        "      see+=1\n",
        "      break\n",
        "      \n",
        "  return total_reward"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5hF5scINGyP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epsilon=0.5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-aCjDbg9Bk1b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#from google.colab import files\n",
        "#files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LOBXs3Dif9X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "checkpoint_directory = \"/train_ckpt\"\n",
        "checkpoint_prefix = os.path.join(checkpoint_directory, \"ckpt\")\n",
        "#checkpoint1 = tf.train.Checkpoint(optimizer=AdamOptimizer(), model=agent)\n",
        "#checkpoint2 = tf.train.Checkpoint(optimizer=AdamOptimizer(), model=agent2)\n",
        "#agent = checkpoint1.restore(tf.train.latest_checkpoint(checkpoint_directory))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2TFUMJ_UyJc",
        "colab_type": "code",
        "outputId": "583ab142-b90d-412a-d646-570cefa0630f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "for i in range(900):\n",
        "  session_rewards= [generate_session(epsilon=epsilon, train=True) for _ in range(10)]\n",
        "  print(\"epoch #{}\\tmean reward = {:.3f}\\tepsilon = {:.3f}\".format(i, np.mean(session_rewards),\n",
        "                                                                   epsilon))\n",
        "  \n",
        "  #if i%100==0:\n",
        "    \n",
        "  #plt.imshow(env.render('rgb_array'))\n",
        "  epsilon*=0.99\n",
        "  assert epsilon >= 1e-4, \"Make sure epsilon is always nonzero during training\"\n",
        "    \n",
        "  if np.mean(session_rewards) < -100 and epsilon < 0.01:\n",
        "      epsilon = 0.3\n",
        "    \n",
        "  if np.mean(session_rewards) > 10:\n",
        "      print (\"You Win!\")\n",
        "      break"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0827 14:25:10.560888 140587525711744 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "W0827 14:25:10.565462 140587525711744 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch #0\tmean reward = -160.497\tepsilon = 0.500\n",
            "epoch #1\tmean reward = -112.251\tepsilon = 0.495\n",
            "epoch #2\tmean reward = -107.006\tepsilon = 0.490\n",
            "epoch #3\tmean reward = -92.332\tepsilon = 0.485\n",
            "epoch #4\tmean reward = -153.139\tepsilon = 0.480\n",
            "epoch #5\tmean reward = -144.048\tepsilon = 0.475\n",
            "epoch #6\tmean reward = -183.708\tepsilon = 0.471\n",
            "epoch #7\tmean reward = -117.410\tepsilon = 0.466\n",
            "epoch #8\tmean reward = -97.610\tepsilon = 0.461\n",
            "epoch #9\tmean reward = -157.431\tepsilon = 0.457\n",
            "epoch #10\tmean reward = -157.725\tepsilon = 0.452\n",
            "epoch #11\tmean reward = -104.724\tepsilon = 0.448\n",
            "epoch #12\tmean reward = -165.584\tepsilon = 0.443\n",
            "epoch #13\tmean reward = -91.212\tepsilon = 0.439\n",
            "epoch #14\tmean reward = -201.187\tepsilon = 0.434\n",
            "epoch #15\tmean reward = -65.806\tepsilon = 0.430\n",
            "epoch #16\tmean reward = -242.337\tepsilon = 0.426\n",
            "epoch #17\tmean reward = -146.046\tepsilon = 0.421\n",
            "epoch #18\tmean reward = -81.212\tepsilon = 0.417\n",
            "epoch #19\tmean reward = -132.893\tepsilon = 0.413\n",
            "epoch #20\tmean reward = -89.627\tepsilon = 0.409\n",
            "epoch #21\tmean reward = -189.650\tepsilon = 0.405\n",
            "epoch #22\tmean reward = -43.763\tepsilon = 0.401\n",
            "epoch #23\tmean reward = -149.320\tepsilon = 0.397\n",
            "epoch #24\tmean reward = -128.326\tepsilon = 0.393\n",
            "epoch #25\tmean reward = -209.989\tepsilon = 0.389\n",
            "epoch #26\tmean reward = -106.212\tepsilon = 0.385\n",
            "epoch #27\tmean reward = -170.780\tepsilon = 0.381\n",
            "epoch #28\tmean reward = -108.802\tepsilon = 0.377\n",
            "epoch #29\tmean reward = -176.544\tepsilon = 0.374\n",
            "epoch #30\tmean reward = -146.748\tepsilon = 0.370\n",
            "epoch #31\tmean reward = -216.253\tepsilon = 0.366\n",
            "epoch #32\tmean reward = -146.479\tepsilon = 0.362\n",
            "epoch #33\tmean reward = -153.892\tepsilon = 0.359\n",
            "epoch #34\tmean reward = -143.254\tepsilon = 0.355\n",
            "epoch #35\tmean reward = -75.496\tepsilon = 0.352\n",
            "epoch #36\tmean reward = -174.491\tepsilon = 0.348\n",
            "epoch #37\tmean reward = -123.681\tepsilon = 0.345\n",
            "epoch #38\tmean reward = -51.345\tepsilon = 0.341\n",
            "epoch #39\tmean reward = -111.662\tepsilon = 0.338\n",
            "epoch #40\tmean reward = -237.281\tepsilon = 0.334\n",
            "epoch #41\tmean reward = -156.610\tepsilon = 0.331\n",
            "epoch #42\tmean reward = -248.327\tepsilon = 0.328\n",
            "epoch #43\tmean reward = -156.504\tepsilon = 0.325\n",
            "epoch #44\tmean reward = -167.964\tepsilon = 0.321\n",
            "epoch #45\tmean reward = -197.159\tepsilon = 0.318\n",
            "epoch #46\tmean reward = -237.653\tepsilon = 0.315\n",
            "epoch #47\tmean reward = -152.985\tepsilon = 0.312\n",
            "epoch #48\tmean reward = -356.432\tepsilon = 0.309\n",
            "epoch #49\tmean reward = -93.000\tepsilon = 0.306\n",
            "epoch #50\tmean reward = -154.575\tepsilon = 0.303\n",
            "epoch #51\tmean reward = -309.840\tepsilon = 0.299\n",
            "epoch #52\tmean reward = -182.251\tepsilon = 0.296\n",
            "epoch #53\tmean reward = -167.491\tepsilon = 0.294\n",
            "epoch #54\tmean reward = -244.162\tepsilon = 0.291\n",
            "epoch #55\tmean reward = -268.645\tepsilon = 0.288\n",
            "epoch #56\tmean reward = -160.977\tepsilon = 0.285\n",
            "epoch #57\tmean reward = -202.122\tepsilon = 0.282\n",
            "epoch #58\tmean reward = -154.547\tepsilon = 0.279\n",
            "epoch #59\tmean reward = -278.907\tepsilon = 0.276\n",
            "epoch #60\tmean reward = -215.912\tepsilon = 0.274\n",
            "epoch #61\tmean reward = -261.532\tepsilon = 0.271\n",
            "epoch #62\tmean reward = -209.104\tepsilon = 0.268\n",
            "epoch #63\tmean reward = -213.841\tepsilon = 0.265\n",
            "epoch #64\tmean reward = -175.233\tepsilon = 0.263\n",
            "epoch #65\tmean reward = -167.809\tepsilon = 0.260\n",
            "epoch #66\tmean reward = -130.894\tepsilon = 0.258\n",
            "epoch #67\tmean reward = -202.894\tepsilon = 0.255\n",
            "epoch #68\tmean reward = -217.267\tepsilon = 0.252\n",
            "epoch #69\tmean reward = -215.722\tepsilon = 0.250\n",
            "epoch #70\tmean reward = -145.524\tepsilon = 0.247\n",
            "epoch #71\tmean reward = -241.429\tepsilon = 0.245\n",
            "epoch #72\tmean reward = -115.994\tepsilon = 0.242\n",
            "epoch #73\tmean reward = -278.047\tepsilon = 0.240\n",
            "epoch #74\tmean reward = -394.969\tepsilon = 0.238\n",
            "epoch #75\tmean reward = -94.028\tepsilon = 0.235\n",
            "epoch #76\tmean reward = -335.121\tepsilon = 0.233\n",
            "epoch #77\tmean reward = -381.681\tepsilon = 0.231\n",
            "epoch #78\tmean reward = -572.150\tepsilon = 0.228\n",
            "epoch #79\tmean reward = -649.494\tepsilon = 0.226\n",
            "epoch #80\tmean reward = -450.533\tepsilon = 0.224\n",
            "epoch #81\tmean reward = -375.012\tepsilon = 0.222\n",
            "epoch #82\tmean reward = -477.472\tepsilon = 0.219\n",
            "epoch #83\tmean reward = -465.546\tepsilon = 0.217\n",
            "epoch #84\tmean reward = -465.074\tepsilon = 0.215\n",
            "epoch #85\tmean reward = -555.941\tepsilon = 0.213\n",
            "epoch #86\tmean reward = -564.108\tepsilon = 0.211\n",
            "epoch #87\tmean reward = -284.969\tepsilon = 0.209\n",
            "epoch #88\tmean reward = -534.654\tepsilon = 0.206\n",
            "epoch #89\tmean reward = -310.592\tepsilon = 0.204\n",
            "epoch #90\tmean reward = -380.764\tepsilon = 0.202\n",
            "epoch #91\tmean reward = -313.714\tepsilon = 0.200\n",
            "epoch #92\tmean reward = -410.423\tepsilon = 0.198\n",
            "epoch #93\tmean reward = -479.410\tepsilon = 0.196\n",
            "epoch #94\tmean reward = -355.106\tepsilon = 0.194\n",
            "epoch #95\tmean reward = -309.387\tepsilon = 0.192\n",
            "epoch #96\tmean reward = -348.000\tepsilon = 0.191\n",
            "epoch #97\tmean reward = -288.849\tepsilon = 0.189\n",
            "epoch #98\tmean reward = -335.020\tepsilon = 0.187\n",
            "epoch #99\tmean reward = -569.968\tepsilon = 0.185\n",
            "epoch #100\tmean reward = -574.257\tepsilon = 0.183\n",
            "epoch #101\tmean reward = -411.668\tepsilon = 0.181\n",
            "epoch #102\tmean reward = -331.241\tepsilon = 0.179\n",
            "epoch #103\tmean reward = -463.048\tepsilon = 0.178\n",
            "epoch #104\tmean reward = -518.644\tepsilon = 0.176\n",
            "epoch #105\tmean reward = -541.467\tepsilon = 0.174\n",
            "epoch #106\tmean reward = -249.486\tepsilon = 0.172\n",
            "epoch #107\tmean reward = -388.812\tepsilon = 0.171\n",
            "epoch #108\tmean reward = -520.359\tepsilon = 0.169\n",
            "epoch #109\tmean reward = -423.153\tepsilon = 0.167\n",
            "epoch #110\tmean reward = -440.248\tepsilon = 0.166\n",
            "epoch #111\tmean reward = -429.559\tepsilon = 0.164\n",
            "epoch #112\tmean reward = -315.799\tepsilon = 0.162\n",
            "epoch #113\tmean reward = -465.780\tepsilon = 0.161\n",
            "epoch #114\tmean reward = -263.963\tepsilon = 0.159\n",
            "epoch #115\tmean reward = -474.533\tepsilon = 0.157\n",
            "epoch #116\tmean reward = -305.300\tepsilon = 0.156\n",
            "epoch #117\tmean reward = -413.447\tepsilon = 0.154\n",
            "epoch #118\tmean reward = -418.155\tepsilon = 0.153\n",
            "epoch #119\tmean reward = -472.047\tepsilon = 0.151\n",
            "epoch #120\tmean reward = -514.926\tepsilon = 0.150\n",
            "epoch #121\tmean reward = -356.424\tepsilon = 0.148\n",
            "epoch #122\tmean reward = -240.079\tepsilon = 0.147\n",
            "epoch #123\tmean reward = -460.576\tepsilon = 0.145\n",
            "epoch #124\tmean reward = -348.818\tepsilon = 0.144\n",
            "epoch #125\tmean reward = -406.457\tepsilon = 0.142\n",
            "epoch #126\tmean reward = -409.971\tepsilon = 0.141\n",
            "epoch #127\tmean reward = -423.319\tepsilon = 0.140\n",
            "epoch #128\tmean reward = -476.159\tepsilon = 0.138\n",
            "epoch #129\tmean reward = -415.573\tepsilon = 0.137\n",
            "epoch #130\tmean reward = -343.688\tepsilon = 0.135\n",
            "epoch #131\tmean reward = -438.658\tepsilon = 0.134\n",
            "epoch #132\tmean reward = -238.671\tepsilon = 0.133\n",
            "epoch #133\tmean reward = -444.572\tepsilon = 0.131\n",
            "epoch #134\tmean reward = -377.607\tepsilon = 0.130\n",
            "epoch #135\tmean reward = -288.352\tepsilon = 0.129\n",
            "epoch #136\tmean reward = -252.690\tepsilon = 0.127\n",
            "epoch #137\tmean reward = -249.735\tepsilon = 0.126\n",
            "epoch #138\tmean reward = -352.020\tepsilon = 0.125\n",
            "epoch #139\tmean reward = -231.455\tepsilon = 0.124\n",
            "epoch #140\tmean reward = -229.523\tepsilon = 0.122\n",
            "epoch #141\tmean reward = -226.656\tepsilon = 0.121\n",
            "epoch #142\tmean reward = -262.541\tepsilon = 0.120\n",
            "epoch #143\tmean reward = -169.991\tepsilon = 0.119\n",
            "epoch #144\tmean reward = -191.516\tepsilon = 0.118\n",
            "epoch #145\tmean reward = -171.030\tepsilon = 0.116\n",
            "epoch #146\tmean reward = -143.549\tepsilon = 0.115\n",
            "epoch #147\tmean reward = -253.627\tepsilon = 0.114\n",
            "epoch #148\tmean reward = -197.708\tepsilon = 0.113\n",
            "epoch #149\tmean reward = -180.596\tepsilon = 0.112\n",
            "epoch #150\tmean reward = -187.399\tepsilon = 0.111\n",
            "epoch #151\tmean reward = -262.181\tepsilon = 0.110\n",
            "epoch #152\tmean reward = -419.500\tepsilon = 0.109\n",
            "epoch #153\tmean reward = -212.195\tepsilon = 0.107\n",
            "epoch #154\tmean reward = -243.624\tepsilon = 0.106\n",
            "epoch #155\tmean reward = -247.555\tepsilon = 0.105\n",
            "epoch #156\tmean reward = -174.273\tepsilon = 0.104\n",
            "epoch #157\tmean reward = -288.913\tepsilon = 0.103\n",
            "epoch #158\tmean reward = -247.528\tepsilon = 0.102\n",
            "epoch #159\tmean reward = -262.021\tepsilon = 0.101\n",
            "epoch #160\tmean reward = -243.590\tepsilon = 0.100\n",
            "epoch #161\tmean reward = -317.264\tepsilon = 0.099\n",
            "epoch #162\tmean reward = -280.564\tepsilon = 0.098\n",
            "epoch #163\tmean reward = -232.333\tepsilon = 0.097\n",
            "epoch #164\tmean reward = -250.116\tepsilon = 0.096\n",
            "epoch #165\tmean reward = -210.969\tepsilon = 0.095\n",
            "epoch #166\tmean reward = -253.497\tepsilon = 0.094\n",
            "epoch #167\tmean reward = -222.450\tepsilon = 0.093\n",
            "epoch #168\tmean reward = -191.301\tepsilon = 0.092\n",
            "epoch #169\tmean reward = -158.160\tepsilon = 0.091\n",
            "epoch #170\tmean reward = -144.390\tepsilon = 0.091\n",
            "epoch #171\tmean reward = -241.265\tepsilon = 0.090\n",
            "epoch #172\tmean reward = -207.047\tepsilon = 0.089\n",
            "epoch #173\tmean reward = -252.035\tepsilon = 0.088\n",
            "epoch #174\tmean reward = -148.916\tepsilon = 0.087\n",
            "epoch #175\tmean reward = -161.864\tepsilon = 0.086\n",
            "epoch #176\tmean reward = -175.038\tepsilon = 0.085\n",
            "epoch #177\tmean reward = -228.179\tepsilon = 0.084\n",
            "epoch #178\tmean reward = -276.657\tepsilon = 0.084\n",
            "epoch #179\tmean reward = -341.263\tepsilon = 0.083\n",
            "epoch #180\tmean reward = -278.655\tepsilon = 0.082\n",
            "epoch #181\tmean reward = -339.566\tepsilon = 0.081\n",
            "epoch #182\tmean reward = -281.323\tepsilon = 0.080\n",
            "epoch #183\tmean reward = -228.031\tepsilon = 0.079\n",
            "epoch #184\tmean reward = -251.674\tepsilon = 0.079\n",
            "epoch #185\tmean reward = -239.502\tepsilon = 0.078\n",
            "epoch #186\tmean reward = -189.982\tepsilon = 0.077\n",
            "epoch #187\tmean reward = -308.087\tepsilon = 0.076\n",
            "epoch #188\tmean reward = -319.521\tepsilon = 0.076\n",
            "epoch #189\tmean reward = -254.124\tepsilon = 0.075\n",
            "epoch #190\tmean reward = -236.778\tepsilon = 0.074\n",
            "epoch #191\tmean reward = -233.653\tepsilon = 0.073\n",
            "epoch #192\tmean reward = -337.262\tepsilon = 0.073\n",
            "epoch #193\tmean reward = -243.910\tepsilon = 0.072\n",
            "epoch #194\tmean reward = -194.286\tepsilon = 0.071\n",
            "epoch #195\tmean reward = -255.787\tepsilon = 0.070\n",
            "epoch #196\tmean reward = -357.766\tepsilon = 0.070\n",
            "epoch #197\tmean reward = -298.500\tepsilon = 0.069\n",
            "epoch #198\tmean reward = -193.654\tepsilon = 0.068\n",
            "epoch #199\tmean reward = -206.184\tepsilon = 0.068\n",
            "epoch #200\tmean reward = -261.114\tepsilon = 0.067\n",
            "epoch #201\tmean reward = -233.411\tepsilon = 0.066\n",
            "epoch #202\tmean reward = -463.596\tepsilon = 0.066\n",
            "epoch #203\tmean reward = -377.540\tepsilon = 0.065\n",
            "epoch #204\tmean reward = -307.156\tepsilon = 0.064\n",
            "epoch #205\tmean reward = -331.647\tepsilon = 0.064\n",
            "epoch #206\tmean reward = -374.020\tepsilon = 0.063\n",
            "epoch #207\tmean reward = -364.498\tepsilon = 0.062\n",
            "epoch #208\tmean reward = -272.786\tepsilon = 0.062\n",
            "epoch #209\tmean reward = -176.119\tepsilon = 0.061\n",
            "epoch #210\tmean reward = -285.838\tepsilon = 0.061\n",
            "epoch #211\tmean reward = -225.139\tepsilon = 0.060\n",
            "epoch #212\tmean reward = -306.927\tepsilon = 0.059\n",
            "epoch #213\tmean reward = -343.484\tepsilon = 0.059\n",
            "epoch #214\tmean reward = -260.957\tepsilon = 0.058\n",
            "epoch #215\tmean reward = -228.230\tepsilon = 0.058\n",
            "epoch #216\tmean reward = -280.937\tepsilon = 0.057\n",
            "epoch #217\tmean reward = -283.725\tepsilon = 0.056\n",
            "epoch #218\tmean reward = -243.575\tepsilon = 0.056\n",
            "epoch #219\tmean reward = -256.216\tepsilon = 0.055\n",
            "epoch #220\tmean reward = -260.559\tepsilon = 0.055\n",
            "epoch #221\tmean reward = -302.827\tepsilon = 0.054\n",
            "epoch #222\tmean reward = -180.060\tepsilon = 0.054\n",
            "epoch #223\tmean reward = -286.375\tepsilon = 0.053\n",
            "epoch #224\tmean reward = -279.683\tepsilon = 0.053\n",
            "epoch #225\tmean reward = -278.491\tepsilon = 0.052\n",
            "epoch #226\tmean reward = -220.333\tepsilon = 0.052\n",
            "epoch #227\tmean reward = -235.347\tepsilon = 0.051\n",
            "epoch #228\tmean reward = -210.231\tepsilon = 0.051\n",
            "epoch #229\tmean reward = -260.453\tepsilon = 0.050\n",
            "epoch #230\tmean reward = -248.996\tepsilon = 0.050\n",
            "epoch #231\tmean reward = -296.716\tepsilon = 0.049\n",
            "epoch #232\tmean reward = -220.243\tepsilon = 0.049\n",
            "epoch #233\tmean reward = -323.518\tepsilon = 0.048\n",
            "epoch #234\tmean reward = -277.251\tepsilon = 0.048\n",
            "epoch #235\tmean reward = -254.351\tepsilon = 0.047\n",
            "epoch #236\tmean reward = -247.683\tepsilon = 0.047\n",
            "epoch #237\tmean reward = -265.540\tepsilon = 0.046\n",
            "epoch #238\tmean reward = -216.003\tepsilon = 0.046\n",
            "epoch #239\tmean reward = -241.689\tepsilon = 0.045\n",
            "epoch #240\tmean reward = -302.406\tepsilon = 0.045\n",
            "epoch #241\tmean reward = -388.941\tepsilon = 0.044\n",
            "epoch #242\tmean reward = -233.789\tepsilon = 0.044\n",
            "epoch #243\tmean reward = -244.332\tepsilon = 0.043\n",
            "epoch #244\tmean reward = -358.505\tepsilon = 0.043\n",
            "epoch #245\tmean reward = -495.190\tepsilon = 0.043\n",
            "epoch #246\tmean reward = -525.494\tepsilon = 0.042\n",
            "epoch #247\tmean reward = -218.901\tepsilon = 0.042\n",
            "epoch #248\tmean reward = -331.932\tepsilon = 0.041\n",
            "epoch #249\tmean reward = -431.376\tepsilon = 0.041\n",
            "epoch #250\tmean reward = -298.529\tepsilon = 0.041\n",
            "epoch #251\tmean reward = -353.461\tepsilon = 0.040\n",
            "epoch #252\tmean reward = -460.398\tepsilon = 0.040\n",
            "epoch #253\tmean reward = -437.554\tepsilon = 0.039\n",
            "epoch #254\tmean reward = -470.778\tepsilon = 0.039\n",
            "epoch #255\tmean reward = -358.447\tepsilon = 0.039\n",
            "epoch #256\tmean reward = -172.033\tepsilon = 0.038\n",
            "epoch #257\tmean reward = -369.508\tepsilon = 0.038\n",
            "epoch #258\tmean reward = -306.953\tepsilon = 0.037\n",
            "epoch #259\tmean reward = -530.795\tepsilon = 0.037\n",
            "epoch #260\tmean reward = -305.999\tepsilon = 0.037\n",
            "epoch #261\tmean reward = -323.847\tepsilon = 0.036\n",
            "epoch #262\tmean reward = -442.515\tepsilon = 0.036\n",
            "epoch #263\tmean reward = -350.873\tepsilon = 0.036\n",
            "epoch #264\tmean reward = -441.521\tepsilon = 0.035\n",
            "epoch #265\tmean reward = -381.493\tepsilon = 0.035\n",
            "epoch #266\tmean reward = -362.537\tepsilon = 0.035\n",
            "epoch #267\tmean reward = -381.679\tepsilon = 0.034\n",
            "epoch #268\tmean reward = -362.481\tepsilon = 0.034\n",
            "epoch #269\tmean reward = -412.182\tepsilon = 0.033\n",
            "epoch #270\tmean reward = -333.035\tepsilon = 0.033\n",
            "epoch #271\tmean reward = -481.918\tepsilon = 0.033\n",
            "epoch #272\tmean reward = -289.195\tepsilon = 0.032\n",
            "epoch #273\tmean reward = -366.182\tepsilon = 0.032\n",
            "epoch #274\tmean reward = -416.858\tepsilon = 0.032\n",
            "epoch #275\tmean reward = -377.403\tepsilon = 0.032\n",
            "epoch #276\tmean reward = -490.041\tepsilon = 0.031\n",
            "epoch #277\tmean reward = -502.012\tepsilon = 0.031\n",
            "epoch #278\tmean reward = -352.758\tepsilon = 0.031\n",
            "epoch #279\tmean reward = -365.104\tepsilon = 0.030\n",
            "epoch #280\tmean reward = -356.267\tepsilon = 0.030\n",
            "epoch #281\tmean reward = -447.161\tepsilon = 0.030\n",
            "epoch #282\tmean reward = -382.782\tepsilon = 0.029\n",
            "epoch #283\tmean reward = -353.546\tepsilon = 0.029\n",
            "epoch #284\tmean reward = -363.859\tepsilon = 0.029\n",
            "epoch #285\tmean reward = -366.171\tepsilon = 0.029\n",
            "epoch #286\tmean reward = -453.062\tepsilon = 0.028\n",
            "epoch #287\tmean reward = -371.710\tepsilon = 0.028\n",
            "epoch #288\tmean reward = -241.860\tepsilon = 0.028\n",
            "epoch #289\tmean reward = -453.159\tepsilon = 0.027\n",
            "epoch #290\tmean reward = -408.162\tepsilon = 0.027\n",
            "epoch #291\tmean reward = -428.517\tepsilon = 0.027\n",
            "epoch #292\tmean reward = -350.586\tepsilon = 0.027\n",
            "epoch #293\tmean reward = -415.813\tepsilon = 0.026\n",
            "epoch #294\tmean reward = -322.363\tepsilon = 0.026\n",
            "epoch #295\tmean reward = -333.506\tepsilon = 0.026\n",
            "epoch #296\tmean reward = -323.044\tepsilon = 0.026\n",
            "epoch #297\tmean reward = -380.221\tepsilon = 0.025\n",
            "epoch #298\tmean reward = -371.486\tepsilon = 0.025\n",
            "epoch #299\tmean reward = -453.132\tepsilon = 0.025\n",
            "epoch #300\tmean reward = -415.643\tepsilon = 0.025\n",
            "epoch #301\tmean reward = -449.648\tepsilon = 0.024\n",
            "epoch #302\tmean reward = -344.124\tepsilon = 0.024\n",
            "epoch #303\tmean reward = -378.371\tepsilon = 0.024\n",
            "epoch #304\tmean reward = -333.342\tepsilon = 0.024\n",
            "epoch #305\tmean reward = -539.569\tepsilon = 0.023\n",
            "epoch #306\tmean reward = -331.620\tepsilon = 0.023\n",
            "epoch #307\tmean reward = -483.148\tepsilon = 0.023\n",
            "epoch #308\tmean reward = -377.820\tepsilon = 0.023\n",
            "epoch #309\tmean reward = -432.972\tepsilon = 0.022\n",
            "epoch #310\tmean reward = -375.191\tepsilon = 0.022\n",
            "epoch #311\tmean reward = -427.118\tepsilon = 0.022\n",
            "epoch #312\tmean reward = -361.338\tepsilon = 0.022\n",
            "epoch #313\tmean reward = -445.024\tepsilon = 0.022\n",
            "epoch #314\tmean reward = -392.598\tepsilon = 0.021\n",
            "epoch #315\tmean reward = -439.172\tepsilon = 0.021\n",
            "epoch #316\tmean reward = -431.968\tepsilon = 0.021\n",
            "epoch #317\tmean reward = -428.449\tepsilon = 0.021\n",
            "epoch #318\tmean reward = -415.139\tepsilon = 0.020\n",
            "epoch #319\tmean reward = -357.401\tepsilon = 0.020\n",
            "epoch #320\tmean reward = -494.840\tepsilon = 0.020\n",
            "epoch #321\tmean reward = -502.626\tepsilon = 0.020\n",
            "epoch #322\tmean reward = -326.877\tepsilon = 0.020\n",
            "epoch #323\tmean reward = -482.009\tepsilon = 0.019\n",
            "epoch #324\tmean reward = -450.777\tepsilon = 0.019\n",
            "epoch #325\tmean reward = -485.573\tepsilon = 0.019\n",
            "epoch #326\tmean reward = -432.610\tepsilon = 0.019\n",
            "epoch #327\tmean reward = -437.694\tepsilon = 0.019\n",
            "epoch #328\tmean reward = -472.908\tepsilon = 0.019\n",
            "epoch #329\tmean reward = -467.733\tepsilon = 0.018\n",
            "epoch #330\tmean reward = -389.424\tepsilon = 0.018\n",
            "epoch #331\tmean reward = -394.937\tepsilon = 0.018\n",
            "epoch #332\tmean reward = -330.322\tepsilon = 0.018\n",
            "epoch #333\tmean reward = -424.362\tepsilon = 0.018\n",
            "epoch #334\tmean reward = -358.612\tepsilon = 0.017\n",
            "epoch #335\tmean reward = -433.105\tepsilon = 0.017\n",
            "epoch #336\tmean reward = -413.405\tepsilon = 0.017\n",
            "epoch #337\tmean reward = -267.402\tepsilon = 0.017\n",
            "epoch #338\tmean reward = -283.050\tepsilon = 0.017\n",
            "epoch #339\tmean reward = -430.931\tepsilon = 0.017\n",
            "epoch #340\tmean reward = -461.704\tepsilon = 0.016\n",
            "epoch #341\tmean reward = -325.774\tepsilon = 0.016\n",
            "epoch #342\tmean reward = -304.301\tepsilon = 0.016\n",
            "epoch #343\tmean reward = -357.378\tepsilon = 0.016\n",
            "epoch #344\tmean reward = -388.497\tepsilon = 0.016\n",
            "epoch #345\tmean reward = -393.108\tepsilon = 0.016\n",
            "epoch #346\tmean reward = -422.249\tepsilon = 0.015\n",
            "epoch #347\tmean reward = -389.599\tepsilon = 0.015\n",
            "epoch #348\tmean reward = -496.082\tepsilon = 0.015\n",
            "epoch #349\tmean reward = -397.441\tepsilon = 0.015\n",
            "epoch #350\tmean reward = -429.223\tepsilon = 0.015\n",
            "epoch #351\tmean reward = -483.016\tepsilon = 0.015\n",
            "epoch #352\tmean reward = -364.994\tepsilon = 0.015\n",
            "epoch #353\tmean reward = -386.746\tepsilon = 0.014\n",
            "epoch #354\tmean reward = -481.997\tepsilon = 0.014\n",
            "epoch #355\tmean reward = -574.220\tepsilon = 0.014\n",
            "epoch #356\tmean reward = -555.461\tepsilon = 0.014\n",
            "epoch #357\tmean reward = -515.545\tepsilon = 0.014\n",
            "epoch #358\tmean reward = -527.055\tepsilon = 0.014\n",
            "epoch #359\tmean reward = -504.933\tepsilon = 0.014\n",
            "epoch #360\tmean reward = -415.839\tepsilon = 0.013\n",
            "epoch #361\tmean reward = -472.045\tepsilon = 0.013\n",
            "epoch #362\tmean reward = -381.606\tepsilon = 0.013\n",
            "epoch #363\tmean reward = -467.390\tepsilon = 0.013\n",
            "epoch #364\tmean reward = -465.267\tepsilon = 0.013\n",
            "epoch #365\tmean reward = -382.318\tepsilon = 0.013\n",
            "epoch #366\tmean reward = -412.969\tepsilon = 0.013\n",
            "epoch #367\tmean reward = -468.054\tepsilon = 0.013\n",
            "epoch #368\tmean reward = -523.561\tepsilon = 0.012\n",
            "epoch #369\tmean reward = -477.432\tepsilon = 0.012\n",
            "epoch #370\tmean reward = -414.471\tepsilon = 0.012\n",
            "epoch #371\tmean reward = -482.966\tepsilon = 0.012\n",
            "epoch #372\tmean reward = -439.699\tepsilon = 0.012\n",
            "epoch #373\tmean reward = -461.855\tepsilon = 0.012\n",
            "epoch #374\tmean reward = -320.002\tepsilon = 0.012\n",
            "epoch #375\tmean reward = -433.049\tepsilon = 0.012\n",
            "epoch #376\tmean reward = -348.548\tepsilon = 0.011\n",
            "epoch #377\tmean reward = -278.732\tepsilon = 0.011\n",
            "epoch #378\tmean reward = -318.751\tepsilon = 0.011\n",
            "epoch #379\tmean reward = -316.860\tepsilon = 0.011\n",
            "epoch #380\tmean reward = -421.471\tepsilon = 0.011\n",
            "epoch #381\tmean reward = -535.643\tepsilon = 0.011\n",
            "epoch #382\tmean reward = -340.978\tepsilon = 0.011\n",
            "epoch #383\tmean reward = -337.580\tepsilon = 0.011\n",
            "epoch #384\tmean reward = -422.810\tepsilon = 0.011\n",
            "epoch #385\tmean reward = -473.022\tepsilon = 0.010\n",
            "epoch #386\tmean reward = -437.975\tepsilon = 0.010\n",
            "epoch #387\tmean reward = -458.691\tepsilon = 0.010\n",
            "epoch #388\tmean reward = -452.810\tepsilon = 0.010\n",
            "epoch #389\tmean reward = -361.823\tepsilon = 0.010\n",
            "epoch #390\tmean reward = -224.230\tepsilon = 0.300\n",
            "epoch #391\tmean reward = -356.257\tepsilon = 0.297\n",
            "epoch #392\tmean reward = -452.315\tepsilon = 0.294\n",
            "epoch #393\tmean reward = -266.997\tepsilon = 0.291\n",
            "epoch #394\tmean reward = -272.693\tepsilon = 0.288\n",
            "epoch #395\tmean reward = -180.404\tepsilon = 0.285\n",
            "epoch #396\tmean reward = -195.233\tepsilon = 0.282\n",
            "epoch #397\tmean reward = -141.335\tepsilon = 0.280\n",
            "epoch #398\tmean reward = -123.663\tepsilon = 0.277\n",
            "epoch #399\tmean reward = -237.712\tepsilon = 0.274\n",
            "epoch #400\tmean reward = -213.334\tepsilon = 0.271\n",
            "epoch #401\tmean reward = -119.437\tepsilon = 0.269\n",
            "epoch #402\tmean reward = -258.317\tepsilon = 0.266\n",
            "epoch #403\tmean reward = -163.180\tepsilon = 0.263\n",
            "epoch #404\tmean reward = -209.512\tepsilon = 0.261\n",
            "epoch #405\tmean reward = -204.667\tepsilon = 0.258\n",
            "epoch #406\tmean reward = -215.073\tepsilon = 0.255\n",
            "epoch #407\tmean reward = -244.017\tepsilon = 0.253\n",
            "epoch #408\tmean reward = -169.915\tepsilon = 0.250\n",
            "epoch #409\tmean reward = -288.585\tepsilon = 0.248\n",
            "epoch #410\tmean reward = -213.895\tepsilon = 0.245\n",
            "epoch #411\tmean reward = -217.692\tepsilon = 0.243\n",
            "epoch #412\tmean reward = -301.819\tepsilon = 0.240\n",
            "epoch #413\tmean reward = -302.948\tepsilon = 0.238\n",
            "epoch #414\tmean reward = -404.506\tepsilon = 0.236\n",
            "epoch #415\tmean reward = -388.935\tepsilon = 0.233\n",
            "epoch #416\tmean reward = -269.323\tepsilon = 0.231\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e22lmmNG58zo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import glob, io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "from IPython import display as ipythondisplay"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqbA49E-6CAw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym.wrappers\n",
        "env = gym.wrappers.Monitor(gym.make(\"LunarLander-v2\"),directory=\"videos\",force=True)\n",
        "sessions = [generate_session(epsilon=0, train=False) for _ in range(100)]\n",
        "env.close()\n",
        "\n",
        "def show_video():\n",
        "  mp4list = glob.glob('videos/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")\n",
        "\n",
        "show_video()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajP5Jod96LK1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "mp4list = glob.glob('videos/*.mp4')\n",
        "mp4 = str(mp4list[0])\n",
        "files.download(mp4)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}